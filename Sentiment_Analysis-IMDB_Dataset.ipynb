{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24f57a6d",
   "metadata": {},
   "source": [
    "<h1>Sentiment Analysis of IMDb Movie Reviews using LSTM and Keras:</h1> \n",
    "    <p> - A Comprehensive Guide by Aminu Mohammed Twumasi</p>\n",
    "    <hr>\n",
    "\n",
    "<h3>This guide provides a step-by-step approach to sentiment analysis of IMDb movie reviews using LSTM and Keras.</h3> The process involves:\n",
    "\n",
    "<ol>\n",
    "    <li><h4>Data Acquisition:</h4> Downloading the 50K IMDb Movie Review dataset</li>\n",
    "    \n",
    "  <li><h4>Data Preprocessing:</h4> Cleaning and preparing the dataset for analysis</li>\n",
    "    \n",
    "  <li><h4>Sentiment Encoding:</h4> Converting sentiment labels into numerical representations</li>\n",
    "    \n",
    "  <li><h4>Data Splitting:</h4> Dividing the dataset into training and testing sets</li>\n",
    "    \n",
    "  <li><h4>Text Tokenization:</h4> Transforming text reviews into numerical sequences</li>\n",
    "    \n",
    "  <li><h4>Model Framework/Architecture/Building:</h4> Constructing an LSTM-based neural network architecture</li>\n",
    "    \n",
    "  <li><h4>Model Training:</h4> Training the model on the training set</li>\n",
    "    \n",
    "  <li><h4>Model Evaluation:</h4> Assessing the model's performance on the testing set</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b025cba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 21:41:49.854173: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd  # For loading the dataset\n",
    "import numpy as np  # For mathematical operations\n",
    "from nltk.corpus import stopwords  # For accessing a collection of stopwords\n",
    "from sklearn.model_selection import train_test_split  # For splitting the dataset\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer  # For encoding text to integers\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences  # For padding or truncating sequences\n",
    "from tensorflow.keras.models import Sequential  # For building the model\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense  # Defining the model architecture\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint  # For saving the model\n",
    "from tensorflow.keras.models import load_model  # For loading a saved model\n",
    "import re  # For regular expression operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3edffe",
   "metadata": {},
   "source": [
    "<h3>Data Acquisition</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0e3f692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  review sentiment\n",
      "0      One of the other reviewers has mentioned that ...  positive\n",
      "1      A wonderful little production. <br /><br />The...  positive\n",
      "2      I thought this was a wonderful way to spend ti...  positive\n",
      "3      Basically there's a family where a little boy ...  negative\n",
      "4      Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
      "...                                                  ...       ...\n",
      "49995  I thought this movie did a down right good job...  positive\n",
      "49996  Bad plot, bad dialogue, bad acting, idiotic di...  negative\n",
      "49997  I am a Catholic taught in parochial elementary...  negative\n",
      "49998  I'm going to have to disagree with the previou...  negative\n",
      "49999  No one expects the Star Trek movies to be high...  negative\n",
      "\n",
      "[50000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset with Pandas\n",
    "data = pd.read_csv('/home/sir/Documents/IMDB_Dataset.csv')\n",
    "\n",
    "# Preview the dataset\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d84356a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopwords are commonly used words in a sentence that are often ignored by search engines, as they do not add much meaning to the search query. Examples of stopwords include \"the\", \"a\", \"an\", \"of\", etc.\n",
    "# Declare the English stopwords for later use\n",
    "\n",
    "english_stops = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e9b2b0",
   "metadata": {},
   "source": [
    "<h3>Dataset Loading and Cleaning</h3>\n",
    "<hr>\n",
    "The initial dataset contains unclean reviews with HTML tags, numbers, uppercase letters, and punctuation marks. This can negatively impact training. Therefore, within the \"load_dataset()\" function, I load the dataset using pandas and perform preprocessing on the reviews. This involves removing HTML tags, non-alphabetic characters (punctuation and numbers), stop words, and converting all reviews to lowercase.\n",
    "\n",
    "<h3>Sentiment Encoding</h3>\n",
    "<hr>\n",
    "Additionally, within the same function, I encode the sentiments as integers. Negative sentiments are encoded as 0, while positive sentiments are encoded as 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c737048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reviews:\n",
      "0        [one, reviewers, mentioned, watching, oz, epis...\n",
      "1        [a, wonderful, little, production, the, filmin...\n",
      "2        [i, thought, wonderful, way, spend, time, hot,...\n",
      "3        [basically, family, little, boy, jake, thinks,...\n",
      "4        [petter, mattei, love, time, money, visually, ...\n",
      "                               ...                        \n",
      "49995    [i, thought, movie, right, good, job, it, crea...\n",
      "49996    [bad, plot, bad, dialogue, bad, acting, idioti...\n",
      "49997    [i, catholic, taught, parochial, elementary, s...\n",
      "49998    [i, going, disagree, previous, comment, side, ...\n",
      "49999    [no, one, expects, star, trek, movies, high, a...\n",
      "Name: review, Length: 50000, dtype: object\n",
      "\n",
      "\n",
      "Sentiment:\n",
      "0        1\n",
      "1        1\n",
      "2        1\n",
      "3        0\n",
      "4        1\n",
      "        ..\n",
      "49995    1\n",
      "49996    0\n",
      "49997    0\n",
      "49998    0\n",
      "49999    0\n",
      "Name: sentiment, Length: 50000, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def load_dataset():\n",
    "    \"\"\"\n",
    "    Loads the IMDb dataset and preprocesses the reviews and sentiment labels.\n",
    "\n",
    "    Returns:\n",
    "        x_data: The reviews, preprocessed and encoded as integers.\n",
    "        y_data: The sentiment labels, encoded as 0 or 1.\n",
    "    \"\"\"\n",
    "\n",
    "    # Read the IMDb dataset from a CSV file\n",
    "    df = pd.read_csv('/home/sir/Documents/IMDB_Dataset.csv')\n",
    "\n",
    "    # Extract the reviews and sentiment labels\n",
    "    x_data = df['review']\n",
    "    y_data = df['sentiment']\n",
    "\n",
    "    # Preprocess the reviews\n",
    "    # Remove HTML tags\n",
    "    x_data = x_data.replace({'<.*?>': ''}, regex=True)\n",
    "\n",
    "    # Remove non-alphabetical characters\n",
    "    x_data = x_data.replace({'[^A-Za-z]': ' '}, regex=True)\n",
    "\n",
    "    # Remove stop words (e.g., \"the\", \"a\", \"an\")\n",
    "    english_stops = set(stopwords.words('english'))  # Define the list of English stopwords\n",
    "    x_data = x_data.apply(lambda review: [w for w in review.split() if w not in english_stops])\n",
    "\n",
    "    # Convert all words to lowercase\n",
    "    x_data = x_data.apply(lambda review: [w.lower() for w in review])\n",
    "\n",
    "    # Encode the sentiment labels\n",
    "    # Replace 'positive' with 1\n",
    "    y_data = y_data.replace('positive', 1)\n",
    "\n",
    "    # Replace 'negative' with 0\n",
    "    y_data = y_data.replace('negative', 0)\n",
    "\n",
    "    return x_data, y_data\n",
    "\n",
    "# Load the preprocessed reviews and sentiment labels\n",
    "x_data, y_data = load_dataset()\n",
    "\n",
    "# Print the reviews\n",
    "print('Reviews:')\n",
    "print(x_data)\n",
    "print('\\n')\n",
    "\n",
    "# Print the sentiment labels\n",
    "print('Sentiment:')\n",
    "print(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a81628f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reviews:\n",
      "0        [one, reviewers, mentioned, watching, oz, epis...\n",
      "1        [a, wonderful, little, production, the, filmin...\n",
      "2        [i, thought, wonderful, way, spend, time, hot,...\n",
      "3        [basically, family, little, boy, jake, thinks,...\n",
      "4        [petter, mattei, love, time, money, visually, ...\n",
      "                               ...                        \n",
      "49995    [i, thought, movie, right, good, job, it, crea...\n",
      "49996    [bad, plot, bad, dialogue, bad, acting, idioti...\n",
      "49997    [i, catholic, taught, parochial, elementary, s...\n",
      "49998    [i, going, disagree, previous, comment, side, ...\n",
      "49999    [no, one, expects, star, trek, movies, high, a...\n",
      "Name: review, Length: 50000, dtype: object\n",
      "\n",
      "\n",
      "Sentiment:\n",
      "0        1\n",
      "1        1\n",
      "2        1\n",
      "3        0\n",
      "4        1\n",
      "        ..\n",
      "49995    1\n",
      "49996    0\n",
      "49997    0\n",
      "49998    0\n",
      "49999    0\n",
      "Name: sentiment, Length: 50000, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Print the reviews\n",
    "print('Reviews:')\n",
    "print(x_data)\n",
    "print('\\n')\n",
    "\n",
    "# Print the sentiment labels\n",
    "print('Sentiment:')\n",
    "print(y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a33dc1",
   "metadata": {},
   "source": [
    "<h3>Dataset Splitting</h3>\n",
    "<hr>\n",
    "To ensure accurate predictions, I have chosen to split the data into a training set comprising 80% of the data and a testing set comprising 20% of the data. This split is achieved using the \"train_test_split\" method from Scikit-Learn. The method automatically shuffles the dataset, which is necessary because the original dataset lists positive reviews first, followed by negative reviews. By shuffling the data, it is distributed evenly in the model, enhancing prediction accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef68e07e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set:\n",
      "2163     [i, belgium, therefore, english, writing, rath...\n",
      "13817    [as, half, big, fans, trash, horror, resist, g...\n",
      "21006    [gritty, drama, emotionally, powerful, blah, t...\n",
      "38895    [tempest, somewhat, self, indulgent, uneven, d...\n",
      "48257    [if, like, horror, movies, lots, blood, gore, ...\n",
      "                               ...                        \n",
      "26542    [it, really, shame, imdb, let, give, negative,...\n",
      "45826    [well, said, horror, comedy, features, neither...\n",
      "40777    [in, lassie, come, home, national, velvet, the...\n",
      "47972    [there, things, i, never, understand, such, mo...\n",
      "28704    [if, like, occasionally, enjoy, watching, terr...\n",
      "Name: review, Length: 40000, dtype: object\n",
      "\n",
      "\n",
      "29699    [i, give, two, lot, music, otherwise, would, o...\n",
      "24396    [did, movie, makers, even, preview, released, ...\n",
      "22755    [in, group, raf, officers, including, eric, wi...\n",
      "38911    [watching, movie, waste, time, i, tempted, lea...\n",
      "14107    [if, vampire, tales, cup, blood, goth, fest, b...\n",
      "                               ...                        \n",
      "18612    [i, care, anyone, else, says, movie, worst, pi...\n",
      "1584     [why, italians, made, best, movies, made, noth...\n",
      "1007     [in, nordestina, village, middle, nowhere, per...\n",
      "45501    [dekalog, five, interesting, viewing, experien...\n",
      "25771    [this, film, refreshing, change, pace, mindles...\n",
      "Name: review, Length: 10000, dtype: object\n",
      "\n",
      "\n",
      "Train Set\n",
      "2163     1\n",
      "13817    0\n",
      "21006    0\n",
      "38895    1\n",
      "48257    1\n",
      "        ..\n",
      "26542    0\n",
      "45826    0\n",
      "40777    1\n",
      "47972    0\n",
      "28704    0\n",
      "Name: sentiment, Length: 40000, dtype: int64\n",
      "\n",
      "\n",
      "Test Set\n",
      "29699    0\n",
      "24396    0\n",
      "22755    1\n",
      "38911    0\n",
      "14107    1\n",
      "        ..\n",
      "18612    0\n",
      "1584     0\n",
      "1007     1\n",
      "45501    1\n",
      "25771    1\n",
      "Name: sentiment, Length: 10000, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Split the preprocessed data into training and testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2)\n",
    "\n",
    "# Print the training reviews\n",
    "print('Train Set:')\n",
    "print(x_train)\n",
    "print('\\n')\n",
    "\n",
    "# Print the testing reviews\n",
    "print(x_test)\n",
    "print('\\n')\n",
    "\n",
    "# Print the training sentiment labels\n",
    "print('Train Set')\n",
    "print(y_train)\n",
    "print('\\n')\n",
    "\n",
    "# Print the testing sentiment labels\n",
    "print('Test Set')\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5f5e89",
   "metadata": {},
   "source": [
    "<h3>Function to Get Maximum Review Length</h3>\n",
    "<hr>\n",
    "To determine the maximum review length, I have created a function that calculates the mean of all the reviews' length using the \"numpy.mean\" method. This provides a good estimate of the maximum review length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5854584",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_length():\n",
    "    \"\"\"\n",
    "    Determines the maximum length of any review in the training set.\n",
    "\n",
    "    Returns:\n",
    "        int: The maximum length of any review.\n",
    "    \"\"\"\n",
    "\n",
    "    review_length = []  # Initialize an empty list to store review lengths\n",
    "\n",
    "    # Iterate through all the reviews in the training set\n",
    "    for review in x_train:\n",
    "        # Append the length of the current review to the `review_length` list\n",
    "        review_length.append(len(review))\n",
    "\n",
    "    # Calculate the average review length\n",
    "    average_review_length = np.mean(review_length)\n",
    "\n",
    "    # Round the average review length up to the nearest integer\n",
    "    max_length = int(np.ceil(average_review_length))\n",
    "\n",
    "    return max_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6233acf",
   "metadata": {},
   "source": [
    "<h3>Text/Review Tokenization and Padding/Truncation</h3>\n",
    "<hr>\n",
    "To prepare the reviews for input into a neural network, we need to encode them into numeric form. I utilize the \"tensorflow.keras.preprocessing.text.Tokenizer\" class to accomplish this. The \"fit_on_texts\" method is used to automatically index each unique word based on the \"x_train\" data.\n",
    "\n",
    "Both \"x_train\" and \"x_test\" are then converted into sequences of integers using the \"texts_to_sequences\" method.\n",
    "\n",
    "Since the reviews vary in length, we need to ensure that they are of the same length for the neural network. This is achieved by adding padding (0) or truncating words. The \"tensorflow.keras.preprocessing.sequence.pad_sequences\" method is employed for this purpose.\n",
    "\n",
    "You can choose to either pad or truncate the words at the end of a sentence (post) or at the beginning of a sentence (pre)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68bb4079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded X Train\n",
      " [[    1 10988  1490 ...     0     0     0]\n",
      " [  111   207    99 ...     0     0     0]\n",
      " [ 2379   345  2175 ...     0     0     0]\n",
      " ...\n",
      " [   50 11492   123 ... 12359  9955 11238]\n",
      " [   49    89     1 ...     0     0     0]\n",
      " [   56     6  1845 ...     0     0     0]] \n",
      "\n",
      "Encoded X Test\n",
      " [[    1   105    36 ...   973   505  1646]\n",
      " [ 1441     3  1146 ...     0     0     0]\n",
      " [   50   443 15090 ...    54     0     0]\n",
      " ...\n",
      " [   50 45776  1948 ...     1  2299    43]\n",
      " [25550   602   129 ...     0     0     0]\n",
      " [    8     4  2356 ...     0     0     0]] \n",
      "\n",
      "Maximum review length:  130\n"
     ]
    }
   ],
   "source": [
    "# ENCODE REVIEW\n",
    "# Initialize a Tokenizer object from the tensorflow.keras.preprocessing.text module. Set the lower parameter to False since the reviews have already been converted to lowercase during data loading.\n",
    "# Create a Tokenizer object:\n",
    "token = Tokenizer(lower=False) \n",
    "\n",
    "# Use the fit_on_texts() method to fit the Tokenizer on the training reviews. This involves creating a vocabulary of unique words and assigning them numerical indices.\n",
    "# Fit the Tokenizer on the training reviews:\n",
    "token.fit_on_texts(x_train)\n",
    "\n",
    "# Convert the training reviews from text sequences to sequences of integers using the texts_to_sequences() method of the Tokenizer. This replaces each word with its corresponding numerical index from the vocabulary.\n",
    "# Encode the training reviews:\n",
    "x_train = token.texts_to_sequences(x_train)\n",
    "\n",
    "# Similarly, convert the testing reviews from text sequences to sequences of integers using the texts_to_sequences() method. This ensures that the training and testing reviews are encoded consistently.\n",
    "# Encode the testing reviews:\n",
    "x_test = token.texts_to_sequences(x_test)\n",
    "\n",
    "# Call the get_max_length() function to determine the maximum length of any review in the training set. This information is crucial for padding or truncating reviews to a uniform length.\n",
    "# Review Padding and Truncating\n",
    "# Determine the maximum review length:\n",
    "max_length = get_max_length()\n",
    "\n",
    "# Use the pad_sequences() function to pad or truncate the training reviews to the max_length. The padding parameter is set to 'post', indicating that padding should be done at the end of the sequences. The truncating parameter is also set to 'post', indicating that longer sequences should be truncated from the end if necessary.\n",
    "# Pad or truncate the training reviews:\n",
    "x_train = pad_sequences(x_train, maxlen=max_length, padding='post', truncating='post')\n",
    "\n",
    "# Apply the same padding and truncating process to the testing reviews using the pad_sequences() function. This ensures that both training and testing reviews have the same length.\n",
    "# Pad or truncate the testing reviews:\n",
    "x_test = pad_sequences(x_test, maxlen=max_length, padding='post', truncating='post')\n",
    "\n",
    "# Determine the total number of unique words in the vocabulary by calculating the length of the token.word_index dictionary. Add 1 to account for the padding index (0).\n",
    "# Vocabulary Size Calculation\n",
    "# Calculate the total number of unique words:\n",
    "total_words = len(token.word_index) + 1   # add 1 because of 0 padding\n",
    "\n",
    "print('Encoded X Train\\n', x_train, '\\n')\n",
    "print('Encoded X Test\\n', x_test, '\\n')\n",
    "print('Maximum review length: ', max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a37e0d5",
   "metadata": {},
   "source": [
    "<h3>Model Architecture</h3>\n",
    "<hr>\n",
    "The model architecture consists of three main layers: Embedding, LSTM, and Dense.\n",
    "\n",
    "The Embedding layer creates word vectors for each word in the \"word_index\" by analyzing the other words around them. This helps group together words that are related or have similar meanings.\n",
    "\n",
    "The LSTM layer is responsible for making decisions on whether to keep or throw away data by considering the current input, previous output, and previous memory. It comprises several important components, including the Forget Gate, Input Gate, Cell State, and Output Gate.\n",
    "\n",
    "The Dense layer computes the input with the weight matrix and bias (optional) using an activation function. In this work, I use the Sigmoid activation function since the output is only 0 or 1.\n",
    "\n",
    "The optimizer used is Adam, and the loss function is Binary Crossentropy since the output is a binary number (0 or 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "323a9047",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 21:48:33.408769: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 130, 32)           2954304   \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 64)                24832     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,979,201\n",
      "Trainable params: 2,979,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 21:48:33.855337: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-11-08 21:48:33.857197: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-11-08 21:48:33.858838: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    }
   ],
   "source": [
    "EMBED_DIM = 32  # Define the embedding dimension (number of units in the embedding layer)\n",
    "LSTM_OUT = 64  # Define the number of units in the LSTM layer\n",
    "\n",
    "# Initialize a Sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Add an Embedding layer to convert words into vectors\n",
    "model.add(Embedding(total_words, EMBED_DIM, input_length=max_length))\n",
    "    # total_words: The size of the vocabulary (number of unique words)\n",
    "    # EMBED_DIM: The dimensionality of the embedding vectors\n",
    "    # input_length: The maximum length of any review (after padding or truncating)\n",
    "\n",
    "# Add an LSTM layer to learn sequential patterns in the reviews\n",
    "model.add(LSTM(LSTM_OUT))\n",
    "    # LSTM_OUT: The number of units in the LSTM layer\n",
    "\n",
    "# Add a Dense layer to produce a binary probability for sentiment prediction\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "    # 1: The output dimension (1 unit for binary classification)\n",
    "    # activation='sigmoid': Use the sigmoid activation function for binary probability output\n",
    "\n",
    "# Compile the model using the Adam optimizer, binary cross-entropy loss, and accuracy metric\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    # optimizer='adam': Use the Adam optimizer for weight updates\n",
    "    # loss='binary_crossentropy': Use binary cross-entropy loss for binary classification\n",
    "    # metrics=['accuracy']: Evaluate the model's accuracy during training and evaluation\n",
    "\n",
    "# Print a summary of the model architecture\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9921ab2a",
   "metadata": {},
   "source": [
    "<h3>Training Process</h3>\n",
    "<hr>\n",
    "To train the model, we simply need to fit our \"x_train\" (input) and \"y_train\" (output/label) data. In this training process, I employ a mini-batch learning method with a batch size of 128 and 5 epochs.\n",
    "\n",
    "Additionally, I have included a callback named \"checkpoint\" to save the model locally after each epoch if its accuracy has improved compared to the previous epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7544e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARCHITECTURE\n",
    "EMBED_DIM = 32  # Define the embedding dimension (number of units in the embedding layer)\n",
    "LSTM_OUT = 64  # Define the number of units in the LSTM layer\n",
    "\n",
    "# Initialize a Sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Add an Embedding layer to convert words into vectors\n",
    "model.add(Embedding(total_words, EMBED_DIM, input_length=max_length))\n",
    "    # total_words: The size of the vocabulary (number of unique words)\n",
    "    # EMBED_DIM: The dimensionality of the embedding vectors\n",
    "    # input_length: The maximum length of any review (after padding or truncating)\n",
    "\n",
    "# Add an LSTM layer to learn sequential patterns in the reviews\n",
    "model.add(LSTM(LSTM_OUT))\n",
    "    # LSTM_OUT: The number of units in the LSTM layer\n",
    "\n",
    "# Add a Dense layer to produce a binary probability for sentiment prediction\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "    # 1: The output dimension (1 unit for binary classification)\n",
    "    # activation='sigmoid': Use the sigmoid activation function for binary probability output\n",
    "\n",
    "# Compile the model using the Adam optimizer, binary cross-entropy loss, and accuracy metric\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    # optimizer='adam': Use the Adam optimizer for weight updates\n",
    "    # loss='binary_crossentropy': Use binary cross-entropy loss for binary classification\n",
    "    # metrics=['accuracy']: Evaluate the model's accuracy during training and evaluation\n",
    "\n",
    "# Print a summary of the model architecture\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d8255a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.0513 - accuracy: 0.9862\n",
      "Epoch 1: accuracy improved from 0.98480 to 0.98620, saving model to models/LSTM.h5\n",
      "313/313 [==============================] - 101s 324ms/step - loss: 0.0513 - accuracy: 0.9862\n",
      "Epoch 2/5\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.0440 - accuracy: 0.9887\n",
      "Epoch 2: accuracy improved from 0.98620 to 0.98870, saving model to models/LSTM.h5\n",
      "313/313 [==============================] - 98s 313ms/step - loss: 0.0440 - accuracy: 0.9887\n",
      "Epoch 3/5\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.0315 - accuracy: 0.9929\n",
      "Epoch 3: accuracy improved from 0.98870 to 0.99292, saving model to models/LSTM.h5\n",
      "313/313 [==============================] - 75s 240ms/step - loss: 0.0315 - accuracy: 0.9929\n",
      "Epoch 4/5\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.0297 - accuracy: 0.9931\n",
      "Epoch 4: accuracy improved from 0.99292 to 0.99308, saving model to models/LSTM.h5\n",
      "313/313 [==============================] - 75s 238ms/step - loss: 0.0297 - accuracy: 0.9931\n",
      "Epoch 5/5\n",
      "313/313 [==============================] - ETA: 0s - loss: 0.0336 - accuracy: 0.9918\n",
      "Epoch 5: accuracy did not improve from 0.99308\n",
      "313/313 [==============================] - 78s 250ms/step - loss: 0.0336 - accuracy: 0.9918\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f200964da10>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model using the training data\n",
    "model.fit(x_train, y_train, batch_size=128, epochs=5, callbacks=[checkpoint])\n",
    "\n",
    "# Explain the code:\n",
    "\n",
    "# model.fit(x_train, y_train):\n",
    "# This line of code starts the training process of the model. It takes three arguments:\n",
    "# x_train: The encoded training reviews (sequences of integers)\n",
    "# y_train: The sentiment labels for the training reviews (0 for negative, 1 for positive)\n",
    "# batch_size: The number of training examples to process in one batch during training (128 in this case)\n",
    "# epochs: The number of times to iterate through the entire training dataset (5 in this case)\n",
    "\n",
    "# callbacks=[checkpoint]:\n",
    "# This part specifies a callback to use during training. A callback is a function that can be called at specific stages of the training process, such as the end of an epoch.\n",
    "# The checkpoint callback is used to save the model weights at regular intervals. This allows you to save the best model during training and use it for inference later.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69423fac",
   "metadata": {},
   "source": [
    "<h3>Testing and Evaluation</h3>\n",
    "<hr>\n",
    "To evaluate the model, we predict the sentiment using the \"x_test\" data and compare the predictions with the \"y_test\" (expected output) data. We then calculate the accuracy of the model by dividing the number of correct predictions by the total data. \n",
    "\n",
    "The resulting accuracy of the model is 86.63%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "92ad075f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 22:06:27.088356: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-11-08 22:06:27.090431: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-11-08 22:06:27.092142: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79/79 [==============================] - 9s 110ms/step\n",
      "Correct Prediction: 5024\n",
      "Wrong Prediction: 4976\n",
      "Accuracy: 50.239999999999995\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the testing data\n",
    "y_pred = model.predict(x_test, batch_size=128)\n",
    "\n",
    "# Convert the predicted probabilities to class labels (0 or 1)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Initialize a counter for correct predictions\n",
    "true = 0\n",
    "\n",
    "# Iterate through the testing data and compare predictions to actual labels\n",
    "for i, y in enumerate(y_test):\n",
    "    if y == y_pred_classes[i]:\n",
    "        true += 1\n",
    "\n",
    "# Calculate the number of correct and incorrect predictions\n",
    "correct_predictions = true\n",
    "wrong_predictions = len(y_pred_classes) - true\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = correct_predictions / len(y_pred_classes) * 100\n",
    "\n",
    "# Print the number of correct and incorrect predictions\n",
    "print('Correct Prediction: {}'.format(correct_predictions))\n",
    "print('Wrong Prediction: {}'.format(wrong_predictions))\n",
    "\n",
    "# Print the accuracy as a percentage\n",
    "print('Accuracy: {:.2f}%'.format(accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a350cf",
   "metadata": {},
   "source": [
    "<h3> To LOAD THE SAVED MODEL and use it for predicting the sentiment of a movie review statement, you can follow these steps:</h3>\n",
    "<hr>\n",
    "1. Use the loaded model to predict the sentiment of the preprocessed movie review statement. The model will output a probability or a binary value indicating the sentiment (positive or negative).\n",
    "\n",
    "2. Interpret the prediction result based on the output of the model. For example, if the model outputs a probability, you can set a threshold (e.g., 0.5) to classify it as either positive or negative. If the model outputs a binary value, you can directly interpret it as the sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e8d8f05c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 22:07:05.194165: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-11-08 22:07:05.196002: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-11-08 22:07:05.197547: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    }
   ],
   "source": [
    "loaded_model = load_model('models/LSTM.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a9622a",
   "metadata": {},
   "source": [
    "Receives a review as an input to be predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4b561143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movie Review: Nothing was typical about this. Everything was beautifully done in this movie, the story, the flow, the scenario, everything. I highly recommend it for mystery lovers, for anyone who wants to watch a good movie!\n"
     ]
    }
   ],
   "source": [
    "review = str(input('Movie Review: '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "20e977c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned:  Nothing was typical about this Everything was beautifully done in this movie the story the flow the scenario everything I highly recommend it for mystery lovers for anyone who wants to watch a good movie\n",
      "Filtered:  ['nothing typical everything beautifully done movie story flow scenario everything i highly recommend mystery lovers anyone wants watch good movie']\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the testing data\n",
    "y_pred = model.predict(x_test, batch_size=128)\n",
    "\n",
    "# Convert the predicted probabilities to class labels (0 or 1)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Initialize a counter for correct predictions\n",
    "true = 0\n",
    "\n",
    "# Iterate through the testing data and compare predictions to actual labels\n",
    "for i, y in enumerate(y_test):\n",
    "    if y == y_pred_classes[i]:\n",
    "        true += 1\n",
    "\n",
    "# Calculate the number of correct and incorrect predictions\n",
    "correct_predictions = true\n",
    "wrong_predictions = len(y_pred_classes) - true\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = correct_predictions / len(y_pred_classes) * 100\n",
    "\n",
    "# Print the number of correct and incorrect predictions\n",
    "print('Correct Prediction: {}'.format(correct_predictions))\n",
    "print('Wrong Prediction: {}'.format(wrong_predictions))\n",
    "\n",
    "# Print the accuracy as a percentage\n",
    "print('Accuracy: {:.2f}%'.format(accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b636a9e",
   "metadata": {},
   "source": [
    "To predict the sentiment of a movie review statement, we first need to tokenize and encode the words in the statement. In this case, we can use the tokenizer that was previously declared and fitted on the training data. This ensures that the words in the statement are encoded based on the word index that is known by the model.\n",
    "\n",
    "By tokenizing and encoding the words using the existing tokenizer, we can prepare the statement for input into the model and obtain the predicted sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e091b30a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  76  702  173 1182  127    3   13 2685 2652  173    1  455  279  683\n",
      "  1778  151  395   33    9    3    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0]]\n"
     ]
    }
   ],
   "source": [
    "tokenize_words = token.texts_to_sequences(filtered)\n",
    "tokenize_words = pad_sequences(tokenize_words, maxlen=max_length, padding='post', truncating='post')\n",
    "print(tokenize_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eee7ef6",
   "metadata": {},
   "source": [
    "This is the result of the prediction which shows the confidence score of the review statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d1f2255a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 752ms/step\n",
      "[[0.9971558]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 22:09:06.275102: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-11-08 22:09:06.277278: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-11-08 22:09:06.279062: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    }
   ],
   "source": [
    "result = loaded_model.predict(tokenize_words)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c16ad6",
   "metadata": {},
   "source": [
    "<h3>Threshold for Sentiment Prediction</h3>\n",
    "<hr>\n",
    "To interpret the output of the model, you can use a threshold value to determine whether the sentiment is positive or negative. A threshold value of 0.7 is a reasonable choice to classify the sentiment as positive or negative.\n",
    "\n",
    "If the confidence score output by the model is close to 0, then the statement is negative. On the other hand, if the confidence score is close to 1, then the statement is positive. If the confidence score is equal to or greater than 0.7, it is classified as positive, and if it is less than 0.7, it is classified as negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a58a9007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive\n"
     ]
    }
   ],
   "source": [
    "if result >= 0.7:\n",
    "    print('positive')\n",
    "else:\n",
    "    print('negative')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd53cbec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
